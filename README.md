# Natural-Language-Processing-in-5-weeks

## Week 1
### Text Precopressing Techniques and Basics for NLP
- #### Regularization, Text Normalization, stemming, lemmatiziation, tokenization and other basics of NLP.
   - Watch the [videos](https://www.youtube.com/watch?v=hyT-BzLyVdU&list=PLDcmCgguL9rxTEz1Rsy6x5NhlBjI8z3Gz) from video 1 to 13 by
TO courses on YouTube.
   - Read the [2nd chapter](https://web.stanford.edu/~jurafsky/slp3/) on Regularization, Text Normalization and Edit Distance by
Stanford.edu.
   - Read the blog post on [Ultimate Guide to Understand and Implement Natural Language Processing with codes in Python](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/) by
Analytics Vidhya.
   - Read my blog post on [Natural Language Processing:-A Beginner's Introduction](https://soumyadip1995.blogspot.com/2019/05/natural-language-processing-beginners.html)
     - See my [Jupyter NoteBook on the Beginner's introduction](https://github.com/soumyadip1995/NLP/blob/master/Natural_language_processing_A_beginner's_introduction.ipynb) for the code.

 - #### Assignment
   - Look at the Notebooks 1-1 to 3-4 and 8-2 in the [Week 1 Assignment NoteBooks](https://github.com/soumyadip1995/Natural_Language_Processing_in_5_weeks/tree/master/Week%201%20Assignment%20NoteBooks)
   - Then use NLTK to perform stemming, lemmatiziation, tokenization, stopword removal on a dataset of your choice.
       
       
## Week 2
###  Word Embeddings in Natural Language Processing
- #### Embedding Layers, Word2vec, GloVe
  - Read the Blog Post on [A Gentle Introduction to Statistical Language Modeling and Neural Language Models](https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/) by Machine Learning Mastery.
  - Read the Material on [Word2Vec Tutorial - The Skip-Gram Model]() by Stanford.edu CS224N
     - Read the paper titled [Efficient Estimation of Word Representations in Vector Space](https://www.scihive.org/paper/1301.3781)
  - Read the Material on [GloVe: Global Vectors for Word Representation ](http://nlp.stanford.edu/pubs/glove.pdf) by Stanford.edu CS224N
  - Read my blog post on Word-Embeddings [here](https://soumyadip1995.blogspot.com/2019/05/natural-language-processing-word.html)
    - See my [Jupyter Notebook on Word Embeddings](https://github.com/soumyadip1995/NLP/blob/master/NLP_word_embeddings.ipynb) for the code
  
- #### Assignment
  - 3 Assignments Visualize and Implement Word2Vec, Create dependency parser all in PyTorch (they are assigments from the stanford course)
  
 ## Week 3
 ### Lexicons and Language Models (Pre-Deep Learning)
 - #### Lexicons and Pre-deep learning Statistical Language model pre-deep learning ( Hidden Markov Models, Topic Modeling with Latent Dirichlet Allocation)
   - Read the materials by [CSEP 517: Natural Language Processing:-University of Washington, Spring 2017](https://courses.cs.washington.edu/courses/csep517/17sp/slides/lecture1.pdf). Read the Lectures from Text Classifiers to Machine Translation(Lectures 2 to 6).
   -  Read [Your Guide to Latent Dirichlet Allocation](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d) by Lettier.
   -  Watch the video on [Topic Modeling with LDA](https://www.youtube.com/watch?v=ZgyA1Q2ywbM) on Youtube
 - #### Assignment
   - Build Hidden Markov Model for Weather Prediction in PyTorch from [Week 3 Assignment code](https://github.com/soumyadip1995/Natural_Language_Processing_in_5_weeks/tree/master/Week%203%20Assignment%20code)

## Week 4
### Deep Sequence Models
- #### Sequence to Sequence Models,(translation, summarization, question answering), Attention based models and  Deep Semantic Similarity
  - Watch the course [Natural Language Processing](https://www.coursera.org/learn/language-processing) by Coursera, Week 4.
  - Read the blog post on [DSSM (Deep Semantic Similarity Model) - Building in TensorFlow](https://kishorepv.github.io/DSSM/) by Kishore P.V.
  - Read Chapter 10- Sequence Modeling, Recurrent and Recursive Nets  of the [Deep-Learning book by Ian Goodfellow](https://github.com/soumyadip1995/deep-learning-by-ian-goodfellow-full-pdf/blob/master/deeplearningbook.pdf)
- #### Assignment
  - 3 Assignments, create a translator and a summarizer. All sequence to sequence models. In pytorch.

## Week 5
### Dialog Systems and Transfer Learning
- #### Speech Recognition, Dialog Managers, Natural Language Understanding and Transfer Learning
  - Watch the course [Natural Language Processing](https://www.coursera.org/learn/language-processing) by Coursera, Week 5.
  - Read the Material on [Dialog Systems and ChatBots](https://web.stanford.edu/~jurafsky/slp3/24.pdf) by stanford.edu
  - Read the blog post on [NLP-Imagenet](http://ruder.io/nlp-imagenet/) by Sebastian Ruder
  - Read the blog post on [Generalized Language Models](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html) by Lilian Weng.
  - Watch Siraj Raval's video [How to Build a Biomedical Startup](https://www.youtube.com/watch?v=J9kbZ5I8gdM) on Youtube
  - [Transfer learning with BERT/GPT-2/ELMO](http://jalammar.github.io/illustrated-bert/)  by Jay Alammar
 - #### Assignment
   - Try and Replicate my Project [TextBrain:-Building an AI startup using Natural Language Processing](https://github.com/soumyadip1995/TextBrain---Building-an-AI-start-up-using-NLP)
   - Play with [Hugging Face:Pytorch Transformers](https://github.com/huggingface/pytorch-pretrained-BERT#examples) pick 2 models, use it for one of 9 Down-Stream tasks, compare their results.
